# 1st Workshop on Action and Anticipation for Visual Learning
*(in conjunction with ECCV 2016, Amsterdam, Oct 8-16)*

This repository contains the LaTeX template for abstract submission to the workshop. A summary of the themes and call for papers is attached below. 

For more details, see http://vision.cs.utexas.edu/aavl_workshop_eccv16/

## Theme
Progress on several computer vision problems in the last few years has been largely fueled by access to today's largest painstakingly curated and manually labeled datasets. However, intuitively and on the basis of the evidence from evolutionary biology and cognitive science, certain kinds of knowledge are available to an agent continuously acting, moving and monitoring its environment that are not available from observing only orderless, i.i.d "bags of images'' with category labels like today's standard datasets. For instance, such an agent may exploit ordered image sequences  i.e. its observed video stream with freely available image<->image and/or image<->other sensor relationships etc. Such forms of supervision may allow it to discover knowledge not otherwise available through the standard supervised paradigm. In our workshop, we aim to focus on how action, motion and anticipation (in the sense of prediction or forming an expectation of that which is not directly observed) may offer viable and possibly important means of supervision or knowledge discovery for various visual tasks.

## Call for Abstracts
We invite 2-page abstracts describing relevant work that has been recently published, is in progress, or is to be presented at ECCV. Review of the submissions will be double blind. While there will be no formal proceedings, accepted abstracts will be posted on this workshop website. Authors of accepted abstracts will present their work in a poster session at the workshop. We encourage submissions from not only the vision community, but also from machine learning, robotics, cognitive science and other related disciplines. A representative, but not exhaustive list of topics of interest is shown below:</p>
<li> Learning from observer motions accompanying video
<li> Models of dynamics of object trajectories/pose changes in video
<li> Unsupervised representation learning from video
<li> Predictive/anticipatory models of future video frames given past frames
<li> Learning from other sensor streams accompanying video
<li> Generative modeling of unobserved object/scene views conditional on observed views
<li> Reinforcement learning approaches for robotic control from pixels
<li> Temporal coherence modeling for learning from video sequences
<li> Sensorimotor learning with visual sensors
<li> Embodied cognition and learning applied to machines
<li> Inferring/modeling physics and physical properties in video
<li> Active vision

## Submission Instructions

Abstracts must not exceed 2 pages (including references). **Submission deadline will be announced soon**

Submission is via the workshop CMT (https://cmt3.research.microsoft.com/User/Login?ReturnUrl=%2FAAVL2016).
